\setcounter{chapter}{6}
\setcounter{page}{1}
\chapter{Scalar Products}

\section{The Definition}

\begin{definition}
A tensor of type $M \cross M \to K$ is called a
\emph{scalar product} or (\emph{bilinear form}) (i.e. two lower
indices).
\end{definition}

\begin{example}
The dot product $K^n \cross K^n \to K$. Writing $X, Y$ as
$n \cross 1$ columns:
\begin{align*}
((\alpha_1,\dots,\alpha_n),(\beta_1,\dots,\beta_n))
&\mapsto \alpha_1\beta_1 +\dots +\alpha_n\beta_n \\
(X,Y) &\mapsto X^tY.
\end{align*}
\end{example}

\section{Properties of Scalar Products}

\begin{enumerate}
\item
If $(\cdot|\cdot)$ is a scalar product on $M$ with components
$G = (g_{ij})$ wrt basis $\u_i$, if $\x$ has components $X = (\phi^i)$
and $\y$ has components $Y = (\nu^i)$  ($g_{ij} = (\u_i|\u_j)$ and
$(\cdot|\cdot) = g_{ij}u^i\tensor u^j$) then
\begin{align*}
(\x|\y) &= (\phi^i\u_i|\nu^j\u_j) \\
&= \phi^i\nu^j(\u_i|\u_j) \\
&= g_{ij}\phi^i\nu^j \\
&= \left(\begin{array}{ccc}
\phi^1 & \dots & \phi^n
\end{array}\right) \begin{array}[t]({ccc})
g_{11} & \dots & g_{1n} \\
\vdots & {} & \vdots \\
g_{n1} & \dots & g_{nn}
\end{array} \begin{array}[t]({c})
\nu^1 \\ \vdots \\ \nu^n
\end{array} \\
&= X^tGY.
\end{align*}
\begin{note}
The dot product has matrix $I$ wrt $\e_i$, since $\e_i.\e_j =
\delta^i_j$.
\end{note}

\item
If $P = (p^i_j)$ is the transition matrix to new basis $\w_i$ then
new matrix of $(\cdot|\cdot)$ is $Q^tGQ$, where $Q = P^{-1}$.

\begin{exampleproof}
As a tensor with two lower indices, new components of $(\cdot|\cdot)$
are:
\[
q^k_iq^l_jg_{kl} = q^k_ig_{kl}q^l_j = Q^tGQ.
\]
Check:
\[
(PX)^tQ^tGQ(Y) = X^tP^tQ^tGQY = X^tGY.
\]
\qed
\end{exampleproof}

\item
$(\cdot|\cdot)$ is called \emph{symmetric} if
\[
(\x|\y) = (\y|\x)
\]
for all $\x, \y$. This is equivalent to $G$ being a symmetric matrix
$G^t = G$:
\[
g_{ij} = (\u_i|\u_j) = (\u_j|\u_i) = g_{ji}.
\]
A symmetric scalar product defines an \emph{associated quadratic form}
\[
F : M \to K
\]
by
\begin{align*}
F(\x) &= (\x|\x) \\
&= X^tGX \\
&= \left(\begin{array}{ccc}
\xi^1 & \dots & \xi^n
\end{array}\right) \begin{array}[t]({ccc})
g_{11} & \dots & g_{1n} \\
\vdots & {} & \vdots \\
g_{n1} & \dots & g_{nn}
\end{array} \begin{array}[t]({c})
\xi^1 \\ \vdots \\ \xi^n
\end{array} \\
&= g_{ij}\xi^i\xi^j,
\end{align*}
i.e.
\[
F =
\left(\begin{array}{ccc}
u^1 & \dots & u^n
\end{array}\right) \begin{array}[t]({ccc})
g_{11} & \dots & g_{1n} \\
\vdots & {} & \vdots \\
g_{n1} & \dots & g_{nn}
\end{array} \begin{array}[t]({c})
u^1 \\ \vdots \\ u^n
\end{array}
= g_{ij}u^iu^j.
\]
$u^iu^j$ is a product of linear forms, and is a function:
\[
(u^iu^j)(\x) = u^i(\x)u^j(\x).
\]
\end{enumerate}

\begin{example}
If $\x, \y, \z$ are coordinate functions on $M$ then
\begin{align*}
F &=
\left(\begin{array}{ccc}
\x & \y & \z
\end{array}\right) \begin{array}[t]({ccc})
3 & 2 & 3 \\
2 & -7 & -1 \\
3 & -1 & 2
\end{array} \begin{array}[t]({c})
\x \\ \y \\ \z
\end{array} \\
&= 3\x^2 - 7\y^2 + 2\z^2 + 4\x\y + 6\x\z - 2\y\z.
\end{align*}
\end{example}
\medskip
(Thus quadratic form $\equiv$ homogeneous $2^{nd}$ degree polynomial).

The quadratic form $F$ determines the symmetric scalar product 
$(\cdot|\cdot)$ uniquely because:
\begin{align*}
(\x+\y|\x+\y) &= (\x|\x) + (\x|\y) + (\y|\x) + (\y|\y), \\
2(\x|\y) &= F(\x+\y) - F(\x) - F(\y) \quad(\text{ if } 1+1 \not= 0),
\end{align*}
and $g_{ij} = (\u_i|\u_j)$ are called the \emph{components of} $F$ wrt
$\u_i$.

\begin{definition}
$(\cdot|\cdot)$ is called \emph{non-singular} if
\[
(\x|\y) = 0 \text{ for all } \y \in M \implies \x = 0,
\]
i.e.
\[
X^tGY = 0 \text{ for all } Y \in K^n \implies X = 0,
\]
i.e.
\[
X^tG = 0 \implies X = 0,
\]
i.e.
\[ \det G \not= 0.
\]
\end{definition}

\begin{definition}
A tensor field $(\cdot|\cdot)$ with two lower indices on an open set $V
\subset \R^n$:
\[
(\cdot|\cdot) = g_{ij}dy^i\tensor dy^j
\]
(say), $y^i$ coordinates on $V$, is called a \emph{metric tensor} if
\[
(\cdot|\cdot)_\x
\]
is a symmetric non-singular scalar product on $T_{\x}\R^n$ for each
$\x \in V$, i.e.
\[
g_{ij} = g_{ji} \quad\text{and} \quad \det g_{ij} \text{ nowhere zero}.
\]
The associated field $ds^2$ of quadratic forms:
\[
ds^2 = g_{ij}dy^idy^j
\]
is called the \emph{line-element} associated with the metric tensor.
\end{definition}

\begin{example}
On $\R^n$ the \emph{usual metric tensor}
\[
dx\tensor dx + dy\tensor dy,
\]
with line element $ds^2 = (dx)^2 + (dy)^2$, has components
\[
\left(\begin{array}{cc}
1 & 0 \\ 0 & 1
\end{array}\right)
\]
wrt coordinates $x, y$.

If
\[
\v = v^1\frac{\prt}{\prt\x} + v^2\frac{\prt}{\prt\y}, \quad
\w = w^1\frac{\prt}{\prt x} + w^2\frac{\prt}{\prt y}
\]
then
\begin{align*}
(\v|\w) = \left(\begin{array}{cc}
v^1 &  v^2
\end{array}\right) \begin{array}[t]({cc})
1 & 0 \\
0 & 1
\end{array} \begin{array}[t]({c})
w^1 \\ w^2
\end{array} &=
v^1w^1 + v^2w^2 \quad\text{(dot product)} \\
ds^2[\v] = (\v|\v) = (v^1)^2 + (v^2)^2 &=
\|\v\|^2 \quad\text{(Euclidean norm)}.
\end{align*}

If $r, \theta$ are polar coordinates:
\[
x = r\cos\theta, \quad y = r\sin\theta,
\]
then
\begin{align*}
dx &= \cos\theta\,dr - r\sin\theta\,d\theta, \\
dy &= \sin\theta\,dr + r\cos\theta\,d\theta
\end{align*}
and
\begin{align*}
ds^2 &= (dx)^2 + (dy)^2 \\
&= (\cos\theta\,dr - r\sin\theta\,d\theta)^2 +
(\sin\theta\,dr + r\cos\theta\,d\theta)^2 \\
&= (dr)^2 + r^2(d\theta)^2
\end{align*}
has components
\[
\left(\begin{array}{cc}
1 & 0 \\
0 & r^2
\end{array}\right)
\]
wrt coordinates $r, \theta$.

If
\[
\v = \alpha^1\frac{\prt}{\prt r} + \alpha^2\frac{\prt}{\prt\theta},
\quad \w = \beta^1\frac{\prt}{\prt r} + \beta^2\frac{\prt}{\prt\theta}
\]
then
\begin{align*}
(\v|\w) &= \alpha^1\beta^1 + r^2\alpha^2\beta^2, \\
\|\v\|^2 &= (\alpha^1)^2 + r^2(\alpha^2)^2.
\end{align*}
\end{example}

\section{Raising and Lowering Indices}

\begin{definition}
Let $M$ be a finite dimensional vector space with a fixed non-singular
symmetric scalar product $(\cdot|\cdot)$. If $\x \in M$ is a vector
(one upper index), we associate with it
\[
\tilde{x} \in M^{\ast},
\]
a linear form (one lower index) defined by:
\[
\la\tilde{x},\y\ra = (\x|\y) \quad\text{for all } \y \in M.
\]
\end{definition}

We call the operation
\begin{align*}
M &\to M^{\ast} \\
\x &\mapsto \tilde{x}
\end{align*}
\emph{lowering the index}. Thus
\[
\tilde{x} \equiv (\x|\cdot) \equiv \text{ `take scalar product with }
\x^,.
\]

If $\x = \alpha^i\u_i$ has components $\alpha^i$ then $\tilde{x}$ has
components
\[
\alpha_j = \la\tilde{x},\u_j\ra = (\x|\u_j) = (\alpha^i\u_i|\u_j) =
\alpha^i(\u_i|\u_j) = \alpha^ig_{ij}.
\]
Since $(\cdot|\cdot)$ is non-singular, $g_{ij}$ is invertible, with
inverse $g^{ij}$ (say), and we have
\[
\alpha^j = \alpha_ig^{ij}.
\]
Thus
\begin{align*}
M &\to M^{\ast} \\
\x &\mapsto \tilde{x}
\end{align*}
is a linear isomorphism, with inverse
\[
\underset{\sim}{f} \leftarrow f
\]
(say),
called \emph{raising the index}. So
\begin{align*}
\x &= \alpha^i\u_i = \underset{\sim}{f}, \\
\tilde{x} &= \alpha_iu^i = f
\end{align*}
and
\[
(\x|\y) = (\underset{\sim}{f}|\y) = \la f,\y\ra = \la\tilde{x},\y\ra.
\]
\begin{itemize}
\item[]
To lower: contract with $g_{ij} \quad(\alpha_j = \alpha^ig_{ij}$).
\item[]
To raise: contract with $g^{ij} \quad(\alpha^j = \alpha_ig^{ij}$).
\end{itemize}

Let $M \Ta M$ be a linear operator and $(\cdot|\cdot)$ be symmetric. The
matrix of $T$ is:
\[
\alpha^i{}_j = \la u^i,T\u_j\ra,
\]
one up, one down mixed components of T.
\[
\alpha_{ij} = (\u_i|T\u_j),
\]
two down covariant components of T.
\[
\alpha_{ij} = (\u_i|\alpha^k{}_j\u_k) = (\u_i|\u_k)\alpha^k{}_j =
g_{ik}\alpha^k{}_j
\]
(lower by contraction with $g_{ij}$). Therefore
\[
\alpha^i{}_j = g^{ik}\alpha_{kj}
\]
(raise by contraction with $g^{ij}$).

If we take the covariant components $\alpha_{ij}$, and raise the
\emph{second} index we get
\[
\alpha_i{}^j = \alpha_{ik}g^{kj}.
\]

$\alpha_{ij}$ are the components of the tensor $B$ (two lower indices)
defined by:
\[
B(\x,\y) = (\x|T\y),
\]
since
\[
B(\u_i,\u_j) = (\u_i|T\u_j) = \alpha_{ij}.
\]

$\alpha_j{}^i$ are the components of an operator $T^{\ast}$ (one upper
index, one lower index) defined by:
\[
(T^{\ast}\x|\y) = (\x|T\y),
\]
since $T^{\ast}$ has components
\[
\gamma_{ij} = (\u_i|T^{\ast}\u_j) = (T^{\ast}\u_j|\u_i) = (\u_j|T\u_i)
= \alpha_{ji},
\]
and therefore $T^{\ast}$ has mixed components:
\[
\gamma^i{}_j = g^{ik}\gamma_{kj} = \alpha_{jk}g^{ki} = \alpha_j{}^i.
\]
$T^{\ast}$ is called the \emph{adjoint} of operator $T$.

\section{Orthogonality and Diagonal Matrix}

\begin{definition}
If $(\cdot|\cdot)$ is a scalar product on $M$ and
\[
(\x|\y) = 0,
\]
we say that $\x$ is \emph{orthogonal to} $\y$ wrt $(\cdot|\cdot)$.

If $N$ is a vector subspace of $M$, we write
\[
N^{\bot} = \{\x \in M : (\x|\y) = 0 \text{ for all } \y \in N\},
\]
and call it the \emph{orthogonal complement of} $N$ wrt $(\cdot|\cdot)$
(see Figure 7.1).

\[
\includegraphics{p055.eps}
\]

\vspace{-74mm} \hspace{+74mm} $\bot$

\vspace{+60mm}

\[
\text{Figure 7.1}
\]

We denote by $(\cdot|\cdot)_N$ the scalar product on $N$ defined by
\[
(\x|\y)_N = (\x|\y) \quad\text{for all } \x, \y \in N,
\]
and call it the \emph{restriction of $(\cdot|\cdot)$ to $N$}.
\end{definition}

\begin{definition}
Let $N_1,\dots,N_k$ be vector subspaces of a vector space $M$. Then we
write
\[
N_1 +\dots +N_k = \{\x_1+\dots+\x_k : \x_1 \in N_1,\dots,\x_k \in N_k\},
\]
and call it the \emph{sum} of $N_1,\dots,N_k$. Thus
$M = N_1 +\dots +N_k$ iff each $\x \in M$ can be written as a sum
\[
\x = \x_1 +\dots +\x_k, \quad \x_i \in N_i.
\]

We call $M$ a \emph{direct sum} of $N_1,\dots,N_k$, and write
\[
M = N_1 \oplus\dots\oplus N_k
\]
if for each $\x \in M$ there exists \emph{unique} $(\x_1,\dots,\x_k)$
(for example, see Figure 7.2) such that
\[
\x = \x_1 +\dots +\x_k \quad\text{and}\quad \x_i \in N_i.
\]

\[
\epsfig{file=p056.eps,width=150mm}
\]

\vspace{-97mm} \hspace{+51mm} $2$

\vspace{+33mm} \hspace{+94mm} $1$

\vspace{+50mm}

\[
\text{Figure 7.2}
\]

\end{definition}

\begin{theorem}
Let $(\cdot|\cdot)$ be a scalar product on $M$. Let $N$ be a
finite-dimensional vector subspace such that $(\cdot|\cdot)_N$ is
non-singular. Then
\[
M = N \oplus N^{\bot}.
\]
\end{theorem}
\begin{proof}
Let $\x \in M$ (see Figure 7.3). Define $f \in N^{\ast}$ by
\[
\la f,\y\ra = (\x|\y)
\]
for all $\y \in N$.

Since $(\cdot|\cdot)_N$ is non-singular we can raise the index of $f$,
and get a \emph{unique} vector $\z \in N$ such that
\[
\la f,\y\ra = (\z|\y)
\]
for all $\y \in N$, i.e.
\[
(\x|\y) = (\z|\y)
\]
for all $\y \in N$, i.e.
\[
(\x-\z|\y) = 0
\]
for all $\y \in N$, i.e.
\[
\x - \z \in N^{\bot},
\]

\[
\includegraphics{p057.eps}
\]

\[
\text{Figure 7.3}
\]

i.e.
\[
\x = \underset{\in N}{\z} + \underset{\in N^{\bot}}{(\x-\z)}
\]
uniquely, as required.
\qed
\end{proof}

\begin{lemma}
Let $(\cdot|\cdot)$ be a symmetric scalar product, not identically
zero on a vector space $M$ over a field $K$ of characteristic
$\not= 2$. (i.e. $1 + 1 \not= 0$). Then there exists $\x \in M$ such
that
\[
(\x|\x) \not= 0.
\]
\end{lemma}
\begin{proof}
Choose $\x, \y \in M$ such that $(\x|\y) \not= 0$. Then
\[
(\x+\y|\x+\y) = (\x|\x) + (\x|\y) + (\y|\x) + (\y|\y).
\]
Hence $(\x+\y|\x+\y), (\x|\x), (\y|\y)$ are not all zero. Hence result.
\qed
\end{proof}

\begin{theorem}
Let $(\cdot|\cdot)$ be a symmetric scalar product on a
finite-dimensional vector space $M$. Then $M$ has a basis of mutually
orthogonal vectors:
\[
(\u_i|\u_j) = 0 \quad\text{if } i \not= j,
\]
i.e. the scalar product has a \emph{diagonal} matrix
\[
\left( \begin{array}{cccc}
\alpha_1 & 0 & \dots & 0 \\
0 & \alpha_2 & \dots & 0 \\
\vdots & {} & \ddots & \vdots \\
0 & \dots & 0 & \alpha_n
\end{array} \right),
\]
where $\alpha_i = (\u_i|\u_i)$.
\end{theorem}
\begin{proof}
Theorem holds if $(\x|\y) = 0$ for all $\x, \y \in M$. So suppose
$(\cdot|\cdot)$ is not identically zero.

Now we use induction on $\dim M$. Theorem holds if $\dim M = 1$. So
assume $\dim M = n > 1$, and that the theorem holds for all spaces of
dimension less than $n$.

Choose $u_1 \in M$ such that
\[
(\u_1|\u_1) = \alpha_1 \not= 0.
\]
Let $N$ be the subspace generated by $\u_1$.  $(\cdot|\cdot)_N$ has
$1 \cross 1$ matrix $(\alpha_1)$, and therefore
is non-singular. Therefore
\begin{align*}
M &= N \oplus N^{\bot} \\
\dim: n &= 1 + n-1.
\end{align*}
By the induction hypothesis $N^{\bot}$ has basis
\[
\u_2,\dots,\u_n
\]
(say) of mutually orthogonal vectors. Therefore $\u_1,\u_2,\dots,\u_n$
is a basis for $M$ of mutually orthogonal vectors, as required.
\qed
\end{proof}
\medskip

If $M$ is a complex vector space, we can put
\[
\w_i = \frac{\u_i}{\sqrt{\alpha_i}}
\]
for each $\alpha_i>0$. Then $(\w_i|\w_i) = 1$ or $0$, and rearranging
we have a basis wrt which $(\cdot|\cdot)$ has matrix
\[
\left(\begin{array}{cc}
\fbox{$\begin{array}{cccc}
1 & {} & {} & {} \\
{} & 1 & {} & {} \\
{} & {} & \ddots & {} \\
{} & {} & {} & 1
\end{array}$} & 0 \\
0 & \fbox{$\begin{array}{cccc}
0 & {} & {} & {} \\
{} & 0 & {} & {} \\
{} & {} & \ddots & {} \\
{} & {} & {} & 0
\end{array}$}
\end{array}\right)
\]
($r \cross r$ diagonal block top left), and the associated quadratic
form is a sum of squares:
\[
(w^1)^2 +\dots + (w^r)^2.
\]

If $M$ is a real vector space, we can put
\[
\w_i = \begin{cases}
\u_i/\sqrt{\alpha_i} &\alpha_i > 0; \\
\u_i/\sqrt{-\alpha_i} &\alpha_i < 0; \\
\u_i &\alpha_i = 0.
\end{cases}
\]
Then $(\w_i|\w_i) = \pm1 \text{ or } 0$, and rearranging we have a
basis wrt which $(\cdot|\cdot)$ has matrix
\[
\left( \begin{array}{ccc}
\fbox{$\begin{array}{ccc}
1 & {} & {} \\
{} & \ddots & {} \\
{} & {} & 1
\end{array}$} & 0 & 0 \\
0 & \fbox{$\begin{array}{ccc}
-1 & {} & {} \\
{} & \ddots & {} \\
{} & {} & -1
\end{array}$} & 0 \\
0 & 0 & \fbox{$\begin{array}{ccc}
0 & {} & {} \\
{} & \ddots & {} \\
{} & {} & 0
\end{array}$}
\end{array} \right),
\]
and the associated quadratic form is a sum and difference of squares:
\[
(w^1)^2 +\dots + (w^r)^2 - (w^{r+1})^2 -\dots - (w^{r+s})^2.
\]

\begin{example}
Let $(\cdot|\cdot)$ be a scalar product on a 3-dimensional space $M$
which has matrix
\[
A =
\left( \begin{array}{ccc}
4 & 2 & 2 \\
2 & 0 & -1 \\
2 & -1 & -3
\end{array} \right)
\]
wrt a basis with coordinate functions $x, y, z$.

To find new coordinate functions wrt which $(\cdot|\cdot)$ has a
diagonal matrix.
\smallskip
\noindent
\emph{Method}: Take the associated quadratic form
\[
F = 4x^2 - 3z^2 + 4xy + 4xz - 2yz,
\]
and write it as a sum and difference of squares, by `completing squares'.
We have:
\begin{align*}
F &= 4(x^2+xy+xz) - 3z^2 - 2yz \\
&= 4(x+\tfrac{1}{2}y+\tfrac{1}{2}z)^2
- y^2 - z^2 - 2yz - 3z^2 - 2yz \\
&= 4(x+\tfrac{1}{2}y+\tfrac{1}{2}z)^2 - (y^2 + 4yz + 4z^2) \\
&= 4(x+\tfrac{1}{2}y+\tfrac{1}{2}z)^2 - (y + 2z)^2 + 0z^2 \\
&= 4u^2 - v^2 + 0w.
\end{align*}
Therefore $(\cdot|\cdot)$ has diagonal matrix
\[
D =
\left( \begin{array}{ccc}
4 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & 0
\end{array} \right)
\]
wrt to coordinate functions
\begin{align*}
u &= x + \tfrac{1}{2}y + \tfrac{1}{2}z, \\
v &= y + 2z, \\
w &= z.
\end{align*}
The transition matrix is
\[
P =
\left( \begin{array}{ccc}
1 & \frac{1}{2} & \frac{1}{2} \\
0 & 1 & 2 \\
0 & 0 & 1
\end{array} \right).
\]
\emph{Check}: $P^tDP = A$?
\[
\left( \begin{array}{ccc}
1 & 0 & 0 \\
\frac{1}{2} & 1 & 0 \\
\frac{1}{2} & 2 & 1
\end{array} \right) \left( \begin{array}{ccc}
4 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & 0
\end{array} \right) \left( \begin{array}{ccc}
1 & \frac{1}{2} & \frac{1}{2} \\
0 & 1 & 2 \\
0 & 0 & 1
\end{array} \right) = \left( \begin{array}{ccc}
4 & 2 & 2 \\
2 & 0 & -1 \\
2 & -1 & -3
\end{array} \right).
\]
\end{example}

For a symmetric scalar product on a real vector space the number of
$+$ signs, and the number of $-$ signs, when the matrix is diagonalised,
is independent of the coordinates chosen:

\begin{theorem}[Sylvester's Law of Inertia]
Let $\un$ and $\w_1,\dots,\w_n$ be bases for a real vector space, and
let
\begin{align*}
F &= (u^1)^2 +\dots + (u^r)^2 - (u^{r+1})^2 -\dots - (u^{r+s})^2 \\
&= (w^1)^2 +\dots + (w^t)^2 - (w^{t+1})^2 -\dots - (w^{t+k})^2
\end{align*}
be a quadratic form diagonalised by each of the two bases. Then
$r = t$ and $s = k$.
\end{theorem}
\begin{proof}
Suppose $r \not= t, r > t$ (say). The space of solutions of the
$n - r + t$ homogeneous linear equations
\[
u^{r+1} = 0,\dots, u^n = 0, w^1 = 0,\dots, w^t = 0
\]
has dimension at least
\[
n - (n-r+t) = r - t > 0.
\]
Therefore there exists a non-zero solution $\x$ so
\begin{align*}
F(\x) &= (u^1(\x))^2 +\dots + (u^r(\x))^2 > 0 \\
&= -(w^{t+1}(\x))^2 -\dots - (w^{t+k}(\x))^2 \leq 0,
\end{align*}
which is clearly a contradiction. Therefore $r = t$, and similarly
$s = k$.
\qed
\end{proof}

\section{Special Spaces}

\begin{definition}
A real vector space $M$ with a symmetric scalar product $(\cdot|\cdot)$
is called a \emph{Euclidean space} if the associated quadratic form is
\emph{positive definite}, i.e.
\[
F(\x) = (\x|\x) > 0 \quad\text{for all } \x \not= 0,
\]
i.e. there exists basis $\un$ such that $(\cdot|\cdot)$ has matrix
\[
\left( \begin{array}{cccc}
1 & 0 & \dots & 0 \\
0 & 1 & \dots & 0 \\
\vdots & {} & \ddots & \vdots \\
0 & \dots & 0 & 1
\end{array} \right)
\]
(all $+$ signs).
\end{definition}

\begin{align*}
F &= (u^1)^2 +\dots + (u^n)^2, \\
(\u_i|\u_j) &= \delta^i_j,
\end{align*}
i.e. $\un$ is \emph{orthonormal}.

We write
\[
\|\x\| = \sqrt{(\x|\x)} \quad (\x \in M),
\]
and call it the \emph{norm} of $\x$. We have
\[
\|\x+\y\| \leq \|\x\| + \|\y\| \quad\text{({\em Triangle Inequality})}.
\]
Thus $M$ is a normed vector space, and therefore a metric space, and
therefore a toplogical space.

The scalar product also satisfies:
\[
|(\x|\y)| \leq \|\x\|\|\y\| \quad\text{({\em Schwarz Inequality})}.
\]

We define the \emph{angle} $\theta$ between two non-zero vectors
$\x, \y$ by:
\[
\frac{(\x|\y)}{\|\x\|\|\y\|} = \cos\theta \quad (0 \leq \theta \leq \pi)
\]
(see Figure 7.4).

\[
\epsfig{file=p062.eps,width=100mm}
\]

\vspace{-44mm} \hspace{+78mm} \large$\pi$ \normalsize

\vspace{+30mm}

\[
\text{Figure 7.4}
\]

If $M$ is an $n$-dimensional vector space with scalar product having an
orthonormal basis (e.g. a complex vector space or a Euclidean vector space)
then the transition matrix $P$ from one orthonormal basis to another
satisfies:
\[
\underset{\text{new}}{P^tIP} = \underset{\text{old}}{I},
\]
i.e.
\[
P^tP = I
\]
i.e. $P$ is an \emph{orthogonal matrix}, i.e.
\[
\left( \begin{array}{c}
{} \\ \cdots\text{$i^{th}$ col of $P$}\cdots \\ \\ \\
\end{array} \right) \left( \begin{array}{ccc}
{} & \vdots & {} \\
{} & j^{th} & {} \\
{} & \col & {} \\
{} & \text{of $P$} & {}
\end{array} \right) = \left( \begin{array}{cccc}
1 & 0 & \dots & 0 \\
0 & 1 & \dots & 0 \\
\vdots & {} & \ddots & \vdots \\
0 & \dots & 0 & 1
\end{array} \right),
\]
i.e.
\[
(i^{th} \text{ col of $P$}).(j^{th} \text{ col of $P$}) = \delta_{ij},
\]
i.e. the columns of $P$ form an orthonormal basis of $K^n$.

Also
\begin{align*}
P \text{ orthonormal} &\iff P^t = P^{-1} \\
&\iff PP^t = I \\
&\iff \text{ the rows of $P$ form an orthonormal basis of $K^n$}.
\end{align*}

\begin{definition}
A real $4$-dimensional vector space $M$ with scalar product
$(\cdot|\cdot)$ of type $+++-$ is called a \emph{Minkowski space}. A
basis $\u_1,\u_2,\u_3,\u_4$ is called a \emph{Lorentz basis} if wrt
$\u_i$ the scalar product has matrix
\[
\left( \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & -1
\end{array} \right),
\]
i.e.
\[
F = (u^1)^2 + (u^2)^2 + (u^3)^2 - (u^4)^2.
\]
\end{definition}

The transition matrix $P$ from one Lorentz basis to another satisfies:
\[
P^t\left( \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & -1
\end{array} \right)P = \left( \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & -1
\end{array} \right).
\]
Such a matrix $P$ is called a \emph{Lorentz matrix}.

\begin{example}
On $\C^n$ we define the \emph{hermitian dot product} $(\x|\y)$ of vectors
\[
\x = (\alpha_1,\dots,\alpha_n), \quad \y = (\beta_1,\dots,\beta_n)
\]
to be
\[
(\x|\y) = \alpha_1\overline{\beta_1} +\dots + \alpha_n\overline{\beta_n}.
\]
This has the property of being positive definite, since:
\[
(\x|\x) =
\alpha_1\overline{\alpha_1} +\dots + \alpha_n\overline{\alpha_n} =
\|\alpha_1\|^2 +\dots + \|\alpha_n\|^2 > 0 \quad\text{ if $\x \not= 0$}.
\]
\end{example}

More generally:

\begin{definition}
If $M$ is a complex vector space then a \emph{hermitian scalar product}
$(\cdot|\cdot)$ on $M$ is a function
\[
M \cross M \to \C
\]
such that
\begin{enumerate}[(i)]
\item
$(\x+\y|\z) = (\x|\z) + (\y|\z)$,
\item
$(\alpha\x|\z) = \alpha(\x|\z)$,
\item
$(\x|\y+\z) = (\x|y) + (\x|\z)$,
\item
$(\x|\alpha\y) = \overline{\alpha}(\x|\y)$,
\item
$\overline{(\x|\y)} = (\y|\x)$.
\end{enumerate}
(i) and (ii) imply linear in the first variable, (iii) and (iv)
imply conjugate-linear in the second variable, (v) implies
conjugate-symmetric.
\end{definition}

If, in addition,
\[
(\x|\x) > 0
\]
for all $\x \not= 0$ then we call $(\cdot|\cdot)$ a
\emph{positive definite} hermitian scalar product.

\begin{definition}
A complex vector space $M$ with a positive definite hermitian scalar
product $(\cdot|\cdot)$ is called a \emph{Hilbert space}.
\end{definition}

\begin{note}
For a finite dimensional complex space $M$ with an hermitian form
$(\cdot|\cdot)$ we can prove (in exactly the same way as for a real
space with symmetric scalar product):
\begin{enumerate}
\item
There exists basis wrt which $(\cdot|\cdot)$ has matrix
\[
\left(\begin{array}{ccc}
\fbox{$\begin{array}{ccc}
1 & {} & {} \\
{} & \ddots & {} \\
{} & {} & 1 
\end{array}$} & 0 & 0 \\
0 & \fbox{$\begin{array}{ccc}
-1 & {} & {} \\
{} & \ddots & {} \\
{} & {} & -1
\end{array}$} & 0 \\
0 & 0 & \fbox{$\begin{array}{ccc}
0 & {} & {} \\
{} & \ddots & {} \\
{} & {} & 0
\end{array}$}
\end{array}\right).
\]
\item
The number of $+$ signs and the number of $-$ signs are each uniquely
determined by $(\cdot|\cdot)$.
\item
$M$ is a Hilbert space iff all the signs are $+$.
\end{enumerate}
Thus $M$ is a Hilbert space iff $M$ has an orthonormal basis. The
transition matrix $P$ from one orthonormal basis to another satisfies:
\[
\underset{\text{new}}{P^tI\overline{P}} = \underset{\text{old}}{I},
\]
i.e.
\[
P^t\overline{P} = I.
\]
Such a matrix is called a \emph{unitary matrix}.
\end{note}

A Hilbert space $M$ is a normed space, hence a metric space, hence a
topological space if we define:
\[
\|\x\| = \sqrt{(\x|\x)}.
\]

To test how many $+,-$ signs a quadratic form has we can use
determinants:

\begin{example}
\[
F = ax^2 + 2bxy + cy^2
= a\left(x + \frac{b}{a}y\right)^2 + \frac{ac-b^2}{a}y^2
\]
on a 2-dimensional space, with coordinate functions $x,y$ and matrix
$\left(\begin{array}{cc}
a & b \\ b & c
\end{array}\right)$.
Therefore
\begin{eqnarray*}
++ &\iff a > 0, &\left|\begin{array}{cc}
a & b \\ b & c
\end{array}\right| > 0, \\
-- &\iff a < 0, &\left|\begin{array}{cc}
a & b \\ b & c
\end{array}\right| > 0, \\
+- &\iff &\left|\begin{array}{cc}
a & b \\ b & c
\end{array}\right| < 0.
\end{eqnarray*}
\end{example}

More generally:

\begin{theorem}[Jacobi's Theorem]
Let $F$ be a quadratic form on a real vector space $M$, with symmetric
matrix $g_{ij}$ wrt basis $\u_i$. Suppose each of the determinants
\[
\Delta_i = \left|\begin{array}{ccc}
g_{11} & \dots & g_{1i} \\
\vdots & {} & \vdots \\
g_{i1} & \dots & g_{ii}
\end{array}\right|
\]
is non-zero $(i = 1,\dots,n)$. Then there exists a basis $\w_i$ such that
$F$ has matrix
\[
\left(\begin{array}{cccc}
\frac{1}{\Delta_1} & {} & {} & {} \\
{} & \frac{\Delta_1}{\Delta_2} & {} & {} \\
{} & {} & \ddots & {} \\
{} & {} & {} & \frac{\Delta_{n-1}}{\Delta_n}
\end{array}\right),
\]
i.e.
\[
F = \frac{1}{\Delta_1}(w^1)^2 + \frac{\Delta_1}{\Delta_2}(w^2)^2 +\dots
+ \frac{\Delta_{n-1}}{\Delta_n}(w^n)^2.
\]
Thus
\begin{align*}
F \text{ is $+$ve definite} &\iff \Delta_1, \Delta_2,\dots, \Delta_n
\text{ all positive}, \\
F \text{ is $-$ve definite} &\iff \Delta_1 < 0, \Delta_2 > 0,
\Delta_3 < 0,\dots.
\end{align*}
\end{theorem}
\begin{proof}
$F(\x) = (\x|\x)$, where $(\cdot|\cdot)$ is a symmetric scalar product,
$(\u_i|\u_j) = g_{ij}$. Let
\[
N_i = \S(\u_1,\dots,\u_i).
\]
$(\cdot|\cdot)_{N_i}$ is non-singular, since $\Delta_i \not= 0$ for
$i = 1,\dots,n$.

Now
\[
\{0\} \subset N_1 \subset N_2 \subset \dots \subset N_{i-1} \subset N_i
\subset \dots \subset N_n = M.
\]
Therefore
\begin{align*}
N_i &= N_{i-1} \oplus (N_i\intersect N_{i-1}^{\bot}) \\
\dim : i &= (i-1) + 1.
\end{align*}

Choose non-zero $\w_i \in N_i \intersect N_{i-1}^{\bot}$. Then
\[
\w_1,\dots,\w_{i-1},\w_i,\dots,\w_n
\]
are mutually orthogonal, and $\w_i$ is orthogonal to
$\u_1,\dots,\u_{i-1}$. Therefore $\w_i$ is \emph{not} orthogonal to
$\u_i$, since $(\cdot|\cdot)$ is non-singular. Therefore we can choose
$\w_i$ such that $(\u_i|\w_i) = 1$.

It remains to show that
\[
(\w_i|\w_i) = \frac{\Delta_{i-1}}{\Delta_i}.
\]
To do this we write
\[
\lambda_1\u_1 +\dots +\lambda_{i-1}\u_{i-1} + \lambda_i\u_i = \w_i.
\]
Taking scalar product with $\w_i,\u_1,\u_2,\dots,\u_i$ we get:
\begin{align*}
0 +\dots + 0 + \lambda_i &= (\w_i|\w_i) \\
\lambda_1g_{11} +\dots + \lambda_{i-1}g_{1,i-1} + \lambda_ig_{1i} &= 0 \\
\lambda_1g_{21} +\dots + \lambda_{i-1}g_{2,i-1} + \lambda_ig_{2i} &= 0 \\
&\vdots \\
\lambda_1g_{i-1,1} +\dots + \lambda_{i-1}g_{i-1,i-1} + \lambda_ig_{i-1,i}
&= 0 \\
\lambda_1g_{i1} +\dots + \lambda_{i-1}g_{i,i-1} + \lambda_ig_{ii} &= 1
\end{align*}
Therefore
\[
(\w_i|\w_i) = \lambda_i =
\frac{\left|\begin{array}{cccc}
g_{11} & \dots & g_{1,i-1} & 0 \\
\vdots & {} & \vdots & \vdots \\
g_{i-1,1} & \dots & g_{i-1,i-1} & 0 \\
g_{i1} & \dots & g_{i,i-1} & 1
\end{array}\right|}{\left|\begin{array}{cccc}
g_{11} & \dots & g_{1,i-1} & g_{1i} \\
\vdots & {} & \vdots & \vdots \\
g_{i-1,1} & \dots & g_{i-1,i-1} & g_{i-1,i} \\
g_{i1} & \dots & g_{i,i-1} & g_{ii}
\end{array}\right|} =
\frac{\Delta_{i-1}}{\Delta_i},
\]
as requried.
\qed
\end{proof}
\medskip

This has an application in Calculus:

\begin{theorem}[Criteria for local maxima or minima]
Let $f$ be a scalar field on a manifold $X$ such that $df_X = 0$, and
let $y^i$ be coordinates on $X$ at $\a$. Put
\[
\Delta_i = \left|\begin{array}{ccc}
\frac{\prt^2f}{\prt{y^1}^2} & \dots & \frac{\prt^2f}{\prt y^1\prt y^i} \\
\vdots & {} & \vdots \\
\frac{\prt^2f}{\prt y^i\prt y^1} & \dots & \frac{\prt^2}{\prt{y^i}^2}
\end{array}\right|.
\]
Then
\begin{enumerate}
\item
If $\Delta_i(\a) > 0$ for $i = 1,\dots,n$ then there exists open nbd $V$
of $\a$ such that
\[
f(\x) > f(\a) \quad\text{for all } \x \in V,\ \x \not= \a,
\]
i.e. $\a$ is a \emph{local minima} of $f$;
\item
If $\Delta_1(\a) < 0, \Delta_2(\a) > 0, \Delta_3(\a) < 0,\dots$ then
there exists open nbd $V$ of $\a$ such that
\[
f(\x) < f(\a) \quad\text{for all } \x \in V,\ \x \not= \a,
\]
i.e. $\a$ is a \emph{local maxima} of $f$.
\end{enumerate}
\end{theorem}

To make sure that $\|\x\| = \sqrt{(\x|\x)}$ is a norm on a Euclidean
or a Hilbert space we need to show that the triangle inequality holds.

\begin{theorem}
Let $M$ be a Euclidean or a Hilbert space. Then
\begin{enumerate}[(i)]
\item
$|(\x|\y)| \leq \|\x\|\|\y\|$ \quad Schwarz,
\item
$\|\x+\y\| \leq \|\x\| + \|\y\|$ \quad Triangle.
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}[(i)]
\item
Let $\x, \y \in M$. Then
\[
(\x|\y) = |(\x|\y)|e^{i\theta}, \quad (\y|\x) = |(\x|\y)|e^{-i\theta}
\]
(say). So for all $\lambda \in \R$ we have:
\begin{align*}
0 &\leq (\lambda e^{-i\theta}\x+\y|\lambda e^{-i\theta}\x+\y) \\
&= \|\x\|^2\lambda^2 + \lambda e^{-i\theta}(\x|\y) +
\lambda e^{i\theta}(\y|\x) + \|\y\|^2 \\
&= \|\x\|^2 + 2\lambda|(\x|\y)| + \|\y\|^2.
\end{align*}
Therefore
\[
|(\x|\y)|^2 \leq \|\x\|^2\|\y\|^2 \quad (`b^2 \leq 4ac^{,}).
\]
Therefore
\[
|(\x|\y)| \leq \|\x\|\|\y\|.
\]

\item
\begin{align*}
\|\x+\y\|^2 &= (\x+\y|\x+\y) \\
&= \|\x\|^2 + (\x|\y) + (\y|\x) + \|\y\|^2 \\
&\leq \|\x\|^2 + 2\|\x\|\|\y\| + \|\y\|^2 \\
&= (\|\x\|+\|\y\|)^2.
\end{align*}
Therefore
\[
\|\x+\y\| \leq \|\x\| + \|\y\|.
\]
\qed
\end{enumerate}
\end{proof}
